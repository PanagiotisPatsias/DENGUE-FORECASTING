{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-vQoi8JrIay4"
   },
   "source": [
    "# Backtesting and Forecasting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "executionInfo": {
     "elapsed": 3441,
     "status": "ok",
     "timestamp": 1770249485283,
     "user": {
      "displayName": "panagiotis patsias",
      "userId": "00711446773180364977"
     },
     "user_tz": -60
    },
    "id": "gVb5Bsbz1STk"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.ensemble import GradientBoostingRegressor, RandomForestRegressor, AdaBoostRegressor\n",
    "from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8eJ8BIUrIjyj"
   },
   "source": [
    "Load and prepare the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1770249494885,
     "user": {
      "displayName": "panagiotis patsias",
      "userId": "00711446773180364977"
     },
     "user_tz": -60
    },
    "id": "fNBRoCmq1Wav"
   },
   "outputs": [],
   "source": [
    "def load_and_prepare_data():\n",
    "    \"\"\"Load and prepare dengue and SST data.\"\"\"\n",
    "\n",
    "    # Load datasets\n",
    "    DATA_DIR = Path(\" Put your data directory path here \")\n",
    "\n",
    "    dengue_df = pd.read_csv(DATA_DIR / \"infodengue_capitals_subsetBR.csv\")\n",
    "    sst_df = pd.read_csv(DATA_DIR / \"sst_indices.csv\")\n",
    "\n",
    "    # Parse dates\n",
    "    dengue_df['data_iniSE'] = pd.to_datetime(dengue_df['data_iniSE'])\n",
    "    dengue_df['year'] = dengue_df['data_iniSE'].dt.year\n",
    "    dengue_df['quarter'] = dengue_df['data_iniSE'].dt.quarter\n",
    "    dengue_df['year_quarter'] = dengue_df['year'].astype(str) + '-Q' + dengue_df['quarter'].astype(str)\n",
    "\n",
    "    sst_df['date'] = pd.to_datetime(sst_df['YR'].astype(str) + '-' + sst_df['MON'].astype(str) + '-01')\n",
    "    sst_df['quarter'] = sst_df['date'].dt.quarter\n",
    "    sst_df['year'] = sst_df['YR']\n",
    "    sst_df['year_quarter'] = sst_df['year'].astype(str) + '-Q' + sst_df['quarter'].astype(str)\n",
    "\n",
    "    # Aggregate SST quarterly\n",
    "    sst_quarterly = sst_df.groupby('year_quarter').agg({\n",
    "        'NINO1+2': 'mean', 'NINO3': 'mean', 'NINO3.4': 'mean', 'ANOM.3': 'mean',\n",
    "        'year': 'first', 'quarter': 'first'\n",
    "    }).reset_index()\n",
    "    sst_quarterly.columns = ['year_quarter', 'nino12', 'nino3', 'nino34', 'nino34_anom', 'year', 'quarter']\n",
    "\n",
    "    # Aggregate dengue quarterly - TARGET ONLY (no same-quarter stats = no leakage)\n",
    "    quarterly = dengue_df.groupby(['year', 'quarter', 'year_quarter']).agg({\n",
    "        'casos_est': 'sum',\n",
    "    }).reset_index()\n",
    "\n",
    "    # Merge\n",
    "    df = quarterly.merge(sst_quarterly, on='year_quarter', how='left', suffixes=('', '_sst'))\n",
    "    df['year'] = df['year'].fillna(df['year_sst'])\n",
    "    df['quarter'] = df['quarter'].fillna(df['quarter_sst'])\n",
    "    df = df.drop(columns=['year_sst', 'quarter_sst'], errors='ignore')\n",
    "    df = df.sort_values(['year', 'quarter']).reset_index(drop=True)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yXf_Z1__InsC"
   },
   "source": [
    "Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "executionInfo": {
     "elapsed": 12,
     "status": "ok",
     "timestamp": 1770249518266,
     "user": {
      "displayName": "panagiotis patsias",
      "userId": "00711446773180364977"
     },
     "user_tz": -60
    },
    "id": "24F-3ddM1Yge"
   },
   "outputs": [],
   "source": [
    "def create_features(df):\n",
    "    \"\"\"Create features using ONLY past information (no leakage).\"\"\"\n",
    "\n",
    "    # Lag features (1-8 quarters back)\n",
    "    for lag in [1, 2, 3, 4, 5, 6, 7, 8]:\n",
    "        df[f'lag_{lag}'] = df['casos_est'].shift(lag)\n",
    "\n",
    "    # Rolling statistics (from past only - note the shift(1))\n",
    "    for window in [2, 4, 8]:\n",
    "        df[f'roll_mean_{window}'] = df['casos_est'].shift(1).rolling(window, min_periods=1).mean()\n",
    "        df[f'roll_std_{window}'] = df['casos_est'].shift(1).rolling(window, min_periods=1).std()\n",
    "        df[f'roll_max_{window}'] = df['casos_est'].shift(1).rolling(window, min_periods=1).max()\n",
    "        df[f'roll_min_{window}'] = df['casos_est'].shift(1).rolling(window, min_periods=1).min()\n",
    "\n",
    "    # Exponential moving averages\n",
    "    for span in [2, 4, 8]:\n",
    "        df[f'ema_{span}'] = df['casos_est'].shift(1).ewm(span=span, adjust=False).mean()\n",
    "\n",
    "    # Seasonal features (known at forecast time)\n",
    "    df['quarter_sin'] = np.sin(2 * np.pi * df['quarter'].astype(int) / 4)\n",
    "    df['quarter_cos'] = np.cos(2 * np.pi * df['quarter'].astype(int) / 4)\n",
    "    df['is_peak_season'] = ((df['quarter'] == 1) | (df['quarter'] == 2)).astype(int)\n",
    "\n",
    "    # Year-over-year\n",
    "    df['yoy_same_quarter'] = df['casos_est'].shift(4)\n",
    "    df['yoy_growth_rate'] = df['casos_est'].shift(1) / (df['casos_est'].shift(5) + 1)\n",
    "\n",
    "    # Trend\n",
    "    df['year_trend'] = df['year'] - df['year'].min()\n",
    "\n",
    "    # Momentum\n",
    "    df['momentum_1q'] = df['casos_est'].shift(1) - df['casos_est'].shift(2)\n",
    "    df['acceleration'] = df['momentum_1q'] - df['momentum_1q'].shift(1)\n",
    "\n",
    "    # Log transforms\n",
    "    df['log_lag_1'] = np.log1p(df['casos_est'].shift(1))\n",
    "    df['log_lag_4'] = np.log1p(df['casos_est'].shift(4))\n",
    "    df['log_roll_mean_4'] = np.log1p(df['roll_mean_4'])\n",
    "\n",
    "    # Ratio\n",
    "    df['ratio_to_roll_mean'] = df['casos_est'].shift(1) / (df['roll_mean_4'] + 1)\n",
    "\n",
    "    # El Niño lagged\n",
    "    df['nino34_lag1'] = df['nino34'].shift(1)\n",
    "\n",
    "    # Get feature columns\n",
    "    feature_cols = [col for col in df.columns if col not in\n",
    "                    ['year', 'quarter', 'year_quarter', 'casos_est',\n",
    "                     'nino12', 'nino3', 'nino34', 'nino34_anom']]\n",
    "\n",
    "    return df, feature_cols"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RJ9662MOIs8u"
   },
   "source": [
    "Train and Evaluate with expanding cross validation and grind search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "executionInfo": {
     "elapsed": 45,
     "status": "ok",
     "timestamp": 1770250286378,
     "user": {
      "displayName": "panagiotis patsias",
      "userId": "00711446773180364977"
     },
     "user_tz": -60
    },
    "id": "NBb819121c_s"
   },
   "outputs": [],
   "source": [
    "def train_and_evaluate(df, feature_cols, train_years, test_year):\n",
    "    \"\"\"Train model and evaluate on specified test year.\"\"\"\n",
    "\n",
    "    # Split data\n",
    "    train_df = df[df['year'].isin(train_years)].copy()\n",
    "    test_df = df[df['year'] == test_year].copy()\n",
    "\n",
    "    # Get valid features\n",
    "    valid_features = [col for col in feature_cols\n",
    "                      if train_df[col].notna().sum() > len(train_df) * 0.5]\n",
    "\n",
    "    # Prepare data\n",
    "    X_train = train_df[valid_features].fillna(train_df[valid_features].median())\n",
    "    y_train = train_df['casos_est']\n",
    "    X_test = test_df[valid_features].fillna(train_df[valid_features].median())\n",
    "    y_test = test_df['casos_est'].values\n",
    "\n",
    "    # Test multiple models\n",
    "    models = {\n",
    "        'GradientBoosting': GradientBoostingRegressor(\n",
    "            n_estimators=200, max_depth=4, learning_rate=0.05, random_state=42\n",
    "        ),\n",
    "        'RandomForest': RandomForestRegressor(\n",
    "            n_estimators=300, max_depth=6, min_samples_leaf=2, random_state=42\n",
    "        ),\n",
    "        'AdaBoost': AdaBoostRegressor(\n",
    "            n_estimators=100, learning_rate=0.1, random_state=42\n",
    "        ),\n",
    "    }\n",
    "\n",
    "    results = {}\n",
    "    for name, model in models.items():\n",
    "        model.fit(X_train, y_train)\n",
    "        y_pred = np.maximum(0, model.predict(X_test))\n",
    "        results[name] = {\n",
    "            'r2': r2_score(y_test, y_pred),\n",
    "            'mae': mean_absolute_error(y_test, y_pred),\n",
    "            'predictions': y_pred,\n",
    "            'model': model\n",
    "        }\n",
    "\n",
    "    return results, y_test, test_df, valid_features, X_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "48HrcoY7MYe8"
   },
   "source": [
    "Bactesting in Normal year and Post-outbreak year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1567,
     "status": "ok",
     "timestamp": 1770251150683,
     "user": {
      "displayName": "panagiotis patsias",
      "userId": "00711446773180364977"
     },
     "user_tz": -60
    },
    "id": "vpYSytN7nWMa",
    "outputId": "3d09fcef-4ce5-4c04-abf8-9e56568c5786"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "DENGUE FORECASTING - HONEST MODEL (NO DATA LEAKAGE)\n",
      "================================================================================\n",
      "['lag_1', 'lag_2', 'lag_3', 'lag_4', 'lag_5', 'lag_6', 'lag_7', 'lag_8', 'roll_mean_2', 'roll_std_2', 'roll_max_2', 'roll_min_2', 'roll_mean_4', 'roll_std_4', 'roll_max_4', 'roll_min_4', 'roll_mean_8', 'roll_std_8', 'roll_max_8', 'roll_min_8', 'ema_2', 'ema_4', 'ema_8', 'quarter_sin', 'quarter_cos', 'is_peak_season', 'yoy_same_quarter', 'yoy_growth_rate', 'year_trend', 'momentum_1q', 'acceleration', 'log_lag_1', 'log_lag_4', 'log_roll_mean_4', 'ratio_to_roll_mean', 'nino34_lag1']\n",
      "\n",
      " Data: 64 quarters, 36 features\n",
      "\n",
      "================================================================================\n",
      "TEST CASE 1: Predicting 2023 (Normal Year)\n",
      "Training: 2010-2022\n",
      "================================================================================\n",
      "\n",
      "Model                        R²             MAE\n",
      "------------------------------------------------\n",
      "GradientBoosting        -1.1611          34,712\n",
      "RandomForest             0.6112          14,317\n",
      "AdaBoost                -0.4104          27,040\n",
      "\n",
      " Best (by MAE): RandomForest with MAE = 14317 (R² = 0.6112)\n",
      "\n",
      " 2023 Predictions:\n",
      "Quarter               Actual       Predicted           Error\n",
      "------------------------------------------------------------\n",
      "2023-Q1               60,064          52,214           7,850\n",
      "2023-Q2               90,194         111,890         -21,696\n",
      "2023-Q3               22,226          16,027           6,199\n",
      "2023-Q4               36,215          14,692          21,523\n",
      "\n",
      "================================================================================\n",
      "TEST CASE 2: Predicting 2025 (Post-Outbreak Year)\n",
      "Training: 2010-2023 (excluding 2024)\n",
      "================================================================================\n",
      "\n",
      "Model                        R²             MAE\n",
      "------------------------------------------------\n",
      "GradientBoosting        -0.1557          59,901\n",
      "RandomForest            -0.3364          61,774\n",
      "AdaBoost                -0.0291          54,438\n",
      "\n",
      " Best (by MAE): AdaBoost with MAE = 54438 (R² = -0.0291)\n",
      "\n",
      " 2025 Predictions:\n",
      "Quarter               Actual       Predicted           Error\n",
      "------------------------------------------------------------\n",
      "2025-Q1              182,293          71,218         111,075\n",
      "2025-Q2              154,296          92,418          61,878\n",
      "2025-Q3               40,761          13,993          26,768\n",
      "2025-Q4               40,257          22,226          18,031\n",
      "\n",
      "================================================================================\n",
      " TOP 10 FEATURE IMPORTANCE (RandomForest for 2023)\n",
      "================================================================================\n",
      "   acceleration              0.3273 ████████████████\n",
      "   yoy_growth_rate           0.0990 ████\n",
      "   lag_6                     0.0816 ████\n",
      "   momentum_1q               0.0466 ██\n",
      "   roll_min_2                0.0401 ██\n",
      "   is_peak_season            0.0359 █\n",
      "   log_lag_4                 0.0321 █\n",
      "   roll_min_4                0.0310 █\n",
      "   roll_std_8                0.0267 █\n",
      "   lag_4                     0.0266 █\n",
      "\n",
      "================================================================================\n",
      " SUMMARY\n",
      "================================================================================\n",
      "\n",
      "┌─────────────────────────────────────────────────────────────────────────────┐\n",
      "│                    HONEST MODEL PERFORMANCE (NO LEAKAGE)                    │\n",
      "├─────────────────────────────────────────────────────────────────────────────┤\n",
      "│                                                                             │\n",
      "│  NORMAL YEAR (2023):                                                        │\n",
      "│  ├── Best Model (lowest MAE): RandomForest                                    │\n",
      "│  ├── MAE: 14,317 cases                                  │\n",
      "│  └── R² Score: 0.6112                                     │\n",
      "│                                                                             │\n",
      "│  POST-OUTBREAK YEAR (2025):                                                 │\n",
      "│  ├── Best Model (lowest MAE): AdaBoost                                        │\n",
      "│  ├── MAE: 54,438 cases                                  │\n",
      "│  └── R² Score: -0.0291                                     │\n",
      "│                                                                             │\n",
      "│  WHY 2025 PREDICTION IS POOR:                                               │\n",
      "│  • 2024 had unprecedented outbreak (877K cases in Q1)                       │\n",
      "│  • 2025 baseline is 3-5x higher than historical average                     │\n",
      "│  • Model trained on 2010-2023 cannot extrapolate to \"new normal\"            │\n",
      "│                                                                             │\n",
      "│  FEATURES USED (all lagged - no leakage):                                   │\n",
      "│  • Lag features: casos_est from quarters 1-8 back                           │\n",
      "│  • Rolling stats: mean, std, max from past quarters                         │\n",
      "│  • Seasonal: quarter sin/cos, peak season indicator                         │\n",
      "│  • Year-over-year: same quarter previous year                               │\n",
      "│  • Momentum: rate of change between quarters                                │\n",
      "│  • El Niño: NINO3.4 index (lagged)                                          │\n",
      "│                                                                             │\n",
      "└─────────────────────────────────────────────────────────────────────────────┘\n",
      "\n",
      "================================================================================\n",
      "ANALYSIS COMPLETE\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main execution.\"\"\"\n",
    "\n",
    "    print(\"=\" * 80)\n",
    "    print(\"DENGUE FORECASTING - HONEST MODEL (NO DATA LEAKAGE)\")\n",
    "    print(\"=\" * 80)\n",
    "\n",
    "    # Load and prepare data\n",
    "    df = load_and_prepare_data()\n",
    "    df, feature_cols = create_features(df)\n",
    "\n",
    "    print(feature_cols)\n",
    "    print(f\"\\n Data: {len(df)} quarters, {len(feature_cols)} features\")\n",
    "\n",
    "    # TEST CASE 1: Normal year (2023) - Best case scenario\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"TEST CASE 1: Predicting 2023 (Normal Year)\")\n",
    "    print(\"Training: 2010-2022\")\n",
    "    print(\"=\" * 80)\n",
    "\n",
    "    train_years_1 = list(range(2010, 2023))\n",
    "    results_2023, y_test_2023, test_df_2023, valid_features, X_train = train_and_evaluate(\n",
    "        df, feature_cols, train_years_1, 2023\n",
    "    )\n",
    "\n",
    "    print(f\"\\n{'Model':<20} {'R²':>10} {'MAE':>15}\")\n",
    "    print(\"-\" * 48)\n",
    "    for name, res in results_2023.items():\n",
    "        print(f\"{name:<20} {res['r2']:>10.4f} {res['mae']:>15,.0f}\")\n",
    "\n",
    "    # Best model for 2023\n",
    "    best_2023 = min(results_2023.items(), key=lambda x: (x[1]['mae'], -x[1]['r2']))\n",
    "    print(f\"\\n Best (by MAE): {best_2023[0]} with MAE = {best_2023[1]['mae']:.0f} (R² = {best_2023[1]['r2']:.4f})\")\n",
    "\n",
    "    print(\"\\n 2023 Predictions:\")\n",
    "    print(f\"{'Quarter':<12} {'Actual':>15} {'Predicted':>15} {'Error':>15}\")\n",
    "    print(\"-\" * 60)\n",
    "    for i, (_, row) in enumerate(test_df_2023.iterrows()):\n",
    "        pred = best_2023[1]['predictions'][i]\n",
    "        print(\n",
    "            f\"{row['year_quarter']:<12} {y_test_2023[i]:>15,.0f} \"\n",
    "            f\"{pred:>15,.0f} \"\n",
    "            f\"{(y_test_2023[i] - pred):>15,.0f}\"\n",
    "        )\n",
    "\n",
    "    # TEST CASE 2: Post-outbreak year (2025)\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"TEST CASE 2: Predicting 2025 (Post-Outbreak Year)\")\n",
    "    print(\"Training: 2010-2023 (excluding 2024)\")\n",
    "    print(\"=\" * 80)\n",
    "\n",
    "    train_years_2 = list(range(2010, 2024))\n",
    "    results_2025, y_test_2025, test_df_2025, _, _ = train_and_evaluate(\n",
    "        df, feature_cols, train_years_2, 2025\n",
    "    )\n",
    "\n",
    "    print(f\"\\n{'Model':<20} {'R²':>10} {'MAE':>15}\")\n",
    "    print(\"-\" * 48)\n",
    "    for name, res in results_2025.items():\n",
    "        print(f\"{name:<20} {res['r2']:>10.4f} {res['mae']:>15,.0f}\")\n",
    "\n",
    "    # Best model for 2025\n",
    "    best_2025 = min(results_2025.items(), key=lambda x: (x[1]['mae'], -x[1]['r2']))\n",
    "    print(f\"\\n Best (by MAE): {best_2025[0]} with MAE = {best_2025[1]['mae']:.0f} (R² = {best_2025[1]['r2']:.4f})\")\n",
    "\n",
    "    print(\"\\n 2025 Predictions:\")\n",
    "    print(f\"{'Quarter':<12} {'Actual':>15} {'Predicted':>15} {'Error':>15}\")\n",
    "    print(\"-\" * 60)\n",
    "    for i, (_, row) in enumerate(test_df_2025.iterrows()):\n",
    "        pred = best_2025[1]['predictions'][i]\n",
    "        print(\n",
    "            f\"{row['year_quarter']:<12} {y_test_2025[i]:>15,.0f} \"\n",
    "            f\"{pred:>15,.0f} \"\n",
    "            f\"{(y_test_2025[i] - pred):>15,.0f}\"\n",
    "        )\n",
    "\n",
    "    # FEATURE IMPORTANCE\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\" TOP 10 FEATURE IMPORTANCE (RandomForest for 2023)\")\n",
    "    print(\"=\" * 80)\n",
    "\n",
    "    rf_model = results_2023['RandomForest']['model']\n",
    "    importance = pd.DataFrame({\n",
    "        'Feature': valid_features,\n",
    "        'Importance': rf_model.feature_importances_\n",
    "    }).sort_values('Importance', ascending=False)\n",
    "\n",
    "    for _, row in importance.head(10).iterrows():\n",
    "        bar = '█' * int(row['Importance'] * 50)\n",
    "        print(f\"   {row['Feature']:<25} {row['Importance']:.4f} {bar}\")\n",
    "\n",
    "\n",
    "    # SUMMARY\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\" SUMMARY\")\n",
    "    print(\"=\" * 80)\n",
    "\n",
    "    print(f\"\"\"\n",
    "┌─────────────────────────────────────────────────────────────────────────────┐\n",
    "│                    HONEST MODEL PERFORMANCE (NO LEAKAGE)                    │\n",
    "├─────────────────────────────────────────────────────────────────────────────┤\n",
    "│                                                                             │\n",
    "│  NORMAL YEAR (2023):                                                        │\n",
    "│  ├── Best Model (lowest MAE): {best_2023[0]:<20}                            │\n",
    "│  ├── MAE: {best_2023[1]['mae']:,.0f} cases                                  │\n",
    "│  └── R² Score: {best_2023[1]['r2']:.4f}                                     │\n",
    "│                                                                             │\n",
    "│  POST-OUTBREAK YEAR (2025):                                                 │\n",
    "│  ├── Best Model (lowest MAE): {best_2025[0]:<20}                            │\n",
    "│  ├── MAE: {best_2025[1]['mae']:,.0f} cases                                  │\n",
    "│  └── R² Score: {best_2025[1]['r2']:.4f}                                     │\n",
    "│                                                                             │\n",
    "│  WHY 2025 PREDICTION IS POOR:                                               │\n",
    "│  • 2024 had unprecedented outbreak (877K cases in Q1)                       │\n",
    "│  • 2025 baseline is 3-5x higher than historical average                     │\n",
    "│  • Model trained on 2010-2023 cannot extrapolate to \"new normal\"            │\n",
    "│                                                                             │\n",
    "│  FEATURES USED (all lagged - no leakage):                                   │\n",
    "│  • Lag features: casos_est from quarters 1-8 back                           │\n",
    "│  • Rolling stats: mean, std, max from past quarters                         │\n",
    "│  • Seasonal: quarter sin/cos, peak season indicator                         │\n",
    "│  • Year-over-year: same quarter previous year                               │\n",
    "│  • Momentum: rate of change between quarters                                │\n",
    "│  • El Niño: NINO3.4 index (lagged)                                          │\n",
    "│                                                                             │\n",
    "└─────────────────────────────────────────────────────────────────────────────┘\n",
    "\"\"\")\n",
    "\n",
    "    print(\"=\" * 80)\n",
    "    print(\"ANALYSIS COMPLETE\")\n",
    "    print(\"=\" * 80)\n",
    "\n",
    "    return results_2023, results_2025, test_df_2025\n",
    "\n",
    "\n",
    "results_2023, results_2025, test_df_2025 = main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SCG1SyheM1Pk"
   },
   "source": [
    "Forcasting of 2026"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 573,
     "status": "ok",
     "timestamp": 1770251285717,
     "user": {
      "displayName": "panagiotis patsias",
      "userId": "00711446773180364977"
     },
     "user_tz": -60
    },
    "id": "QvF7-CQT3ukY",
    "outputId": "aaa8ee68-7537-43bf-9115-f2192e242d4b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using best model from 2023: RandomForest\n",
      "\n",
      " 2026 Quarterly Forecast (4 steps ahead):\n",
      "  year_quarter  predicted_casos_est\n",
      "0      2026-Q1         81411.121861\n",
      "1      2026-Q2         86399.481258\n",
      "2      2026-Q3         29230.254434\n",
      "3      2026-Q4         29297.428339\n"
     ]
    }
   ],
   "source": [
    "from sklearn.base import clone\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def refit_best_model_and_forecast_2026(df, feature_cols, results_2023, results_2025,\n",
    "                                      use_best_from=\"2023\",\n",
    "                                      exclude_2024=True):\n",
    "    \"\"\"\n",
    "    Refit best model on all data up to 2025 (optionally excluding 2024),\n",
    "    then recursively forecast 2026Q1..2026Q4.\n",
    "    \"\"\"\n",
    "\n",
    "    # Pick best model\n",
    "    if use_best_from == \"2023\":\n",
    "        best_name = max(results_2023.items(), key=lambda x: x[1]['r2'])[0]\n",
    "        base_model = results_2023[best_name]['model']\n",
    "    else:\n",
    "        best_name = max(results_2025.items(), key=lambda x: x[1]['r2'])[0]\n",
    "        base_model = results_2025[best_name]['model']\n",
    "\n",
    "    print(f\"Using best model from {use_best_from}: {best_name}\")\n",
    "\n",
    "\n",
    "    # Build training set up to 2025\n",
    "    train_df = df[df[\"year\"] <= 2025].copy()\n",
    "    if exclude_2024:\n",
    "        train_df = train_df[train_df[\"year\"] != 2024].copy()\n",
    "\n",
    "    # Create features on the training df\n",
    "    train_df_feat, _ = create_features(train_df)\n",
    "\n",
    "    # Keep only rows where target exists\n",
    "    train_df_feat = train_df_feat.dropna(subset=[\"casos_est\"]).copy()\n",
    "\n",
    "    # Keep only valid feature columns that exist\n",
    "    valid_features = [c for c in feature_cols if c in train_df_feat.columns]\n",
    "\n",
    "    # Drop rows with any NaN\n",
    "    X_train = train_df_feat[valid_features].copy()\n",
    "    y_train = train_df_feat[\"casos_est\"].values\n",
    "\n",
    "    all_nan_cols = [c for c in valid_features if X_train[c].isna().all()]\n",
    "    valid_features = [c for c in valid_features if c not in all_nan_cols]\n",
    "    X_train = train_df_feat[valid_features].copy()\n",
    "\n",
    "    # Drop rows with NaNs\n",
    "    mask = ~X_train.isna().any(axis=1)\n",
    "    X_train = X_train.loc[mask]\n",
    "    y_train = y_train[mask.values]\n",
    "\n",
    "    # Refit\n",
    "    model = clone(base_model)\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    # Create future quarters for 2026Q1..Q4\n",
    "    future_rows = []\n",
    "    for q in [1, 2, 3, 4]:\n",
    "        future_rows.append({\n",
    "            \"year\": 2026,\n",
    "            \"quarter\": q,\n",
    "            \"year_quarter\": f\"2026-Q{q}\",\n",
    "            \"casos_est\": np.nan,\n",
    "            # ENSO columns (scenario fill below)\n",
    "            \"nino12\": np.nan,\n",
    "            \"nino3\": np.nan,\n",
    "            \"nino34\": np.nan,\n",
    "            \"nino34_anom\": np.nan,\n",
    "        })\n",
    "\n",
    "    future_df = pd.DataFrame(future_rows)\n",
    "\n",
    "    # Scenario assumption for ENSO in 2026:\n",
    "    # Use last observed values (persistence) from 2025Q4\n",
    "    last_obs = df[df[\"year\"] <= 2025].sort_values([\"year\", \"quarter\"]).iloc[-1]\n",
    "    for col in [\"nino12\", \"nino3\", \"nino34\", \"nino34_anom\"]:\n",
    "        if col in df.columns and pd.notna(last_obs.get(col, np.nan)):\n",
    "            future_df[col] = last_obs[col]\n",
    "\n",
    "    # Combine history + future placeholders\n",
    "    extended = pd.concat([df.copy(), future_df], ignore_index=True)\n",
    "    extended = extended.sort_values([\"year\", \"quarter\"]).reset_index(drop=True)\n",
    "\n",
    "\n",
    "    # Recursive forecast for 2026Q1..Q4\n",
    "    preds = []\n",
    "    for step in range(4):\n",
    "        # Recompute features each step so lags/rolling include previous predictions\n",
    "        ext_feat, _ = create_features(extended)\n",
    "\n",
    "        # Find current quarter row (2026-Q{step+1})\n",
    "        yq = f\"2026-Q{step+1}\"\n",
    "        row_idx = ext_feat.index[ext_feat[\"year_quarter\"] == yq][0]\n",
    "\n",
    "        X_row = ext_feat.loc[[row_idx], valid_features].copy()\n",
    "\n",
    "        medians = X_train.median(numeric_only=True)\n",
    "        X_row = X_row.fillna(medians)\n",
    "\n",
    "        y_pred = float(model.predict(X_row)[0])\n",
    "        preds.append((yq, y_pred))\n",
    "\n",
    "        extended.loc[extended[\"year_quarter\"] == yq, \"casos_est\"] = y_pred\n",
    "\n",
    "    forecast_2026 = pd.DataFrame(preds, columns=[\"year_quarter\", \"predicted_casos_est\"])\n",
    "    return forecast_2026, model, valid_features\n",
    "\n",
    "\n",
    "# RUN FORECAST FOR 2026\n",
    "df = load_and_prepare_data()\n",
    "df, feature_cols = create_features(df)\n",
    "\n",
    "forecast_2026, fitted_model, used_features = refit_best_model_and_forecast_2026(\n",
    "    df=df,\n",
    "    feature_cols=feature_cols,\n",
    "    results_2023=results_2023,\n",
    "    results_2025=results_2025,\n",
    "    use_best_from=\"2023\",\n",
    "    exclude_2024=True\n",
    ")\n",
    "\n",
    "print(\"\\n 2026 Quarterly Forecast (4 steps ahead):\")\n",
    "print(forecast_2026)\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
